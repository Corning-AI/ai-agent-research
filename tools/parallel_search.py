#!/usr/bin/env python3
"""
Parallel GitHub Search Tool
ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œæœç´¢ CADã€Circuit å’Œå…¶ä»–é¢†åŸŸçš„ AI é¡¹ç›®
"""

import json
import subprocess
import sqlite3
import time
from datetime import datetime
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

class ParallelGitHubSearcher:
    """å¹¶è¡Œ GitHub æœç´¢å™¨"""

    def __init__(self, db_path: str = "../data/projects.db"):
        self.db_path = db_path
        self.lock = threading.Lock()
        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
        CREATE TABLE IF NOT EXISTS projects (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            domain TEXT NOT NULL,
            name TEXT NOT NULL,
            full_name TEXT UNIQUE NOT NULL,
            url TEXT NOT NULL,
            stars INTEGER DEFAULT 0,
            forks INTEGER DEFAULT 0,
            watchers INTEGER DEFAULT 0,
            open_issues INTEGER DEFAULT 0,
            description TEXT,
            main_language TEXT,
            tech_stack TEXT,
            last_updated DATE,
            created_at DATE,
            activity_score REAL DEFAULT 0,
            relevance_score REAL DEFAULT 0,
            readme_summary TEXT,
            license TEXT,
            topics TEXT,
            contributors_count INTEGER DEFAULT 0,
            collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(full_name)
        )
        """)

        conn.commit()
        conn.close()
        print(f"âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")

    def search_with_gh(self, query: str, domain: str, limit: int = 30) -> Tuple[str, List[Dict]]:
        """ä½¿ç”¨ gh CLI æœç´¢ä»“åº“"""
        print(f"ğŸ” [{domain}] æœç´¢: {query}")

        cmd = [
            "gh", "search", "repos",
            query,
            "--limit", str(limit),
            "--sort", "stars",
            "--json", "name,owner,stargazersCount,forksCount,description,url,updatedAt,createdAt,language,license,openIssuesCount"
        ]

        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            repos = json.loads(result.stdout)
            print(f"  âœ… [{domain}] æ‰¾åˆ° {len(repos)} ä¸ªé¡¹ç›®")
            return (domain, repos)
        except subprocess.CalledProcessError as e:
            print(f"  âŒ [{domain}] æœç´¢å¤±è´¥: {e.stderr}")
            return (domain, [])
        except json.JSONDecodeError as e:
            print(f"  âŒ [{domain}] JSON è§£æå¤±è´¥: {e}")
            return (domain, [])

    def calculate_activity_score(self, repo: Dict) -> float:
        """è®¡ç®—æ´»è·ƒåº¦åˆ†æ•°"""
        stars = repo.get("stargazersCount", 0)
        forks = repo.get("forksCount", 0)

        try:
            last_updated = datetime.fromisoformat(repo["updatedAt"].replace("Z", "+00:00"))
            days_since_update = (datetime.now(last_updated.tzinfo) - last_updated).days
        except:
            days_since_update = 365

        score = (
            min(stars / 100, 50) +
            min(forks / 20, 20) +
            max(30 - days_since_update / 10, 0)
        )

        return round(min(score, 100), 2)

    def convert_repo_data(self, repo: Dict, domain: str) -> Dict:
        """è½¬æ¢ gh CLI è¾“å‡ºæ ¼å¼åˆ°æ•°æ®åº“æ ¼å¼"""
        owner_login = repo.get("owner", {}).get("login", "")
        repo_name = repo.get("name", "")
        full_name = f"{owner_login}/{repo_name}"

        language = repo.get("language", "")

        license_name = ""
        if repo.get("license"):
            if isinstance(repo["license"], dict):
                license_name = repo["license"].get("name", "")
            else:
                license_name = repo["license"]

        topics = []

        return {
            "domain": domain,
            "name": repo_name,
            "full_name": full_name,
            "url": repo.get("url", f"https://github.com/{full_name}"),
            "stars": repo.get("stargazersCount", 0),
            "forks": repo.get("forksCount", 0),
            "watchers": repo.get("stargazersCount", 0),
            "open_issues": repo.get("openIssuesCount", 0),
            "description": repo.get("description", ""),
            "main_language": language,
            "last_updated": repo.get("updatedAt", "")[:10],
            "created_at": repo.get("createdAt", "")[:10],
            "license": license_name,
            "topics": json.dumps(topics),
            "activity_score": self.calculate_activity_score(repo)
        }

    def save_to_database(self, projects: List[Dict]):
        """æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.lock:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            saved_count = 0
            updated_count = 0

            for project in projects:
                try:
                    cursor.execute("""
                    INSERT INTO projects (
                        domain, name, full_name, url, stars, forks, watchers,
                        open_issues, description, main_language, last_updated,
                        created_at, activity_score, license, topics
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        project["domain"], project["name"], project["full_name"],
                        project["url"], project["stars"], project["forks"],
                        project["watchers"], project["open_issues"],
                        project["description"], project["main_language"],
                        project["last_updated"], project["created_at"],
                        project["activity_score"], project["license"],
                        project["topics"]
                    ))
                    saved_count += 1
                except sqlite3.IntegrityError:
                    cursor.execute("""
                    UPDATE projects SET
                        stars = ?, forks = ?, watchers = ?,
                        open_issues = ?, description = ?,
                        last_updated = ?, activity_score = ?,
                        license = ?, topics = ?,
                        collected_at = CURRENT_TIMESTAMP
                    WHERE full_name = ?
                    """, (
                        project["stars"], project["forks"],
                        project["watchers"], project["open_issues"],
                        project["description"], project["last_updated"],
                        project["activity_score"], project["license"],
                        project["topics"], project["full_name"]
                    ))
                    updated_count += 1

            conn.commit()
            conn.close()

            return saved_count, updated_count

    def parallel_search(self, search_queries: List[Tuple[str, str]], max_workers: int = 10):
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæœç´¢æŸ¥è¯¢"""
        print(f"\nğŸš€ å¯åŠ¨å¹¶è¡Œæœç´¢ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
        print(f"ğŸ“Š æ€»å…± {len(search_queries)} ä¸ªæœç´¢ä»»åŠ¡\n")

        all_results = []
        domain_stats = {}

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æœç´¢ä»»åŠ¡
            future_to_query = {
                executor.submit(self.search_with_gh, query, domain): (query, domain)
                for query, domain in search_queries
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_query):
                query, domain = future_to_query[future]
                try:
                    domain_result, repos = future.result()

                    # è½¬æ¢æ•°æ®å¹¶æ”¶é›†
                    for repo in repos:
                        project = self.convert_repo_data(repo, domain_result)
                        all_results.append(project)

                    # ç»Ÿè®¡
                    if domain_result not in domain_stats:
                        domain_stats[domain_result] = 0
                    domain_stats[domain_result] += len(repos)

                except Exception as exc:
                    print(f"  âŒ æŸ¥è¯¢ '{query}' å¤±è´¥: {exc}")

        return all_results, domain_stats

    def export_to_json(self, domain: str, output_path: str):
        """å¯¼å‡ºæŒ‡å®šé¢†åŸŸçš„æ•°æ®ä¸º JSON"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
        SELECT * FROM projects WHERE domain = ? ORDER BY stars DESC
        """, (domain,))

        columns = [desc[0] for desc in cursor.description]
        rows = cursor.fetchall()

        projects = []
        for row in rows:
            project = dict(zip(columns, row))
            if project["topics"]:
                try:
                    project["topics"] = json.loads(project["topics"])
                except:
                    project["topics"] = []
            projects.append(project)

        conn.close()

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(projects, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“„ å¯¼å‡ºå®Œæˆ: {output_path} ({len(projects)} ä¸ªé¡¹ç›®)")
        return projects


def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        Parallel GitHub Searcher - Multi-Domain AI           â•‘
â•‘           ä½¿ç”¨ 10 ä¸ªå¹¶è¡Œçº¿ç¨‹è¿›è¡Œå¤§è§„æ¨¡è°ƒç ”                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    searcher = ParallelGitHubSearcher(db_path="../data/projects.db")

    # å®šä¹‰æœç´¢æŸ¥è¯¢çŸ©é˜µï¼ˆæŸ¥è¯¢å…³é”®è¯, é¢†åŸŸï¼‰
    search_queries = [
        # CAD é¢†åŸŸ (5ä¸ªæŸ¥è¯¢)
        ("cad ai stars:>50", "cad"),
        ("autocad automation stars:>50", "cad"),
        ("solidworks api stars:>20", "cad"),
        ("fusion360 automation stars:>20", "cad"),
        ("cad machine learning stars:>30", "cad"),

        # Circuit é¢†åŸŸ (5ä¸ªæŸ¥è¯¢)
        ("circuit simulator ai stars:>50", "circuit"),
        ("pcb design automation stars:>50", "circuit"),
        ("spice simulator stars:>50", "circuit"),
        ("electronic design automation stars:>30", "circuit"),
        ("circuit analysis ai stars:>20", "circuit"),

        # è¡¥å…… LaTeX æœç´¢ (5ä¸ªæŸ¥è¯¢)
        ("latex neural network stars:>20", "latex"),
        ("latex template generator stars:>50", "latex"),
        ("academic writing ai stars:>50", "latex"),
        ("bibliography management stars:>100", "latex"),
        ("latex syntax checker stars:>20", "latex"),

        # Framework/é€šç”¨ AI Agent æ¡†æ¶ (5ä¸ªæŸ¥è¯¢)
        ("ai agent framework stars:>100", "framework"),
        ("langchain stars:>100", "framework"),
        ("autogen stars:>50", "framework"),
        ("crewai stars:>50", "framework"),
        ("openai assistant api stars:>50", "framework"),
    ]

    # æ‰§è¡Œå¹¶è¡Œæœç´¢
    start_time = time.time()
    all_projects, domain_stats = searcher.parallel_search(search_queries, max_workers=10)
    elapsed_time = time.time() - start_time

    # å»é‡
    unique_projects = {}
    for project in all_projects:
        unique_projects[project["full_name"]] = project

    final_projects = list(unique_projects.values())
    final_projects.sort(key=lambda x: x["stars"], reverse=True)

    print(f"\n{'='*70}")
    print(f"ğŸ“Š æœç´¢ç»Ÿè®¡:")
    print(f"   æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"   æ€»è®¡æ‰¾åˆ°: {len(all_projects)} ä¸ªé¡¹ç›®ï¼ˆå«é‡å¤ï¼‰")
    print(f"   å»é‡å: {len(final_projects)} ä¸ªå”¯ä¸€é¡¹ç›®")
    print(f"\n   å„é¢†åŸŸåˆ†å¸ƒ:")
    for domain, count in sorted(domain_stats.items()):
        print(f"      {domain}: {count} ä¸ªé¡¹ç›®")

    # ä¿å­˜åˆ°æ•°æ®åº“
    saved, updated = searcher.save_to_database(final_projects)
    print(f"\nğŸ’¾ æ•°æ®åº“ä¿å­˜: âœ… æ–°å¢ {saved} | ğŸ”„ æ›´æ–° {updated}")

    # æŒ‰é¢†åŸŸå¯¼å‡º JSON
    import os
    for domain in set(p["domain"] for p in final_projects):
        os.makedirs(f"../data/{domain}", exist_ok=True)
        searcher.export_to_json(domain, f"../data/{domain}/projects.json")

    # æ˜¾ç¤º Top 10
    print(f"\nğŸ† Top 10 è·¨é¢†åŸŸ AI é¡¹ç›®:")
    print("="*80)
    for i, project in enumerate(final_projects[:10], 1):
        desc = project['description'][:60] + "..." if project['description'] else "æ— æè¿°"
        print(f"{i:2d}. [{project['domain']:8s}] â­ {project['stars']:6d} | {project['full_name']}")
        print(f"    {desc}")
        print(f"    è¯­è¨€: {project['main_language'] or 'N/A':12s} | æ›´æ–°: {project['last_updated']} | æ´»è·ƒåº¦: {project['activity_score']}")
        print()

    print("âœ… å¹¶è¡Œæœç´¢å®Œæˆï¼")
    print(f"\nğŸ“ æ•°æ®å·²ä¿å­˜:")
    print(f"   - æ•°æ®åº“: ../data/projects.db")
    for domain in set(p["domain"] for p in final_projects):
        print(f"   - JSON ({domain}): ../data/{domain}/projects.json")


if __name__ == "__main__":
    main()
